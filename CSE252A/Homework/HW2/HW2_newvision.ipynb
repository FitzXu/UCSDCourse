{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 252A Computer Vision I Fall 2018 - Assignment 2\n",
    "\n",
    "### Instructor: David Kriegman\n",
    "### Assignment Published On: Wednesday, October 24, 2018\n",
    "### Due On: Wednesday, November 7, 2018 11:59 pm\n",
    "\n",
    "## Instructions\n",
    "* Review the academic integrity and collaboration policies on the course website.\n",
    "* This assignment must be completed individually.\n",
    "* This assignment contains theoretical and programming exercises. If you plan to submit hand written answers for theoretical exercises, please be sure your writing is readable and merge those in order with the final pdf you create out of this notebook. You could fill the answers within the notebook iteself by creating a markdown cell.\n",
    "* Programming aspects of this assignment must be completed using Python in this notebook.\n",
    "* If you want to modify the skeleton code, you can do so. This has been provided just to provide you with a framework for the solution.\n",
    "* You may use python packages for basic linear algebra (you can use numpy or scipy for basic operations), but you may not use packages that directly solve the problem.\n",
    "* If you are unsure about using a specific package or function, then ask the instructor and teaching assistants for clarification.\n",
    "* You must submit this notebook exported as a pdf. You must also submit this notebook as .ipynb file.\n",
    "* You must submit both files (.pdf and .ipynb) on Gradescope. You must mark each problem on Gradescope in the pdf.\n",
    "* **Late policy** - 10% per day late penalty after due date up to 3 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Steradians [2 pts]\n",
    "\n",
    "Calculate the number of steradians contained in a spherical wedge with radius $r = 1$, defined by $\\theta=\\frac{\\pi}{6}$, $\\phi=\\frac{\\pi}{6}$ centered around vector $(\\frac{\\sqrt{2}}{4}, \\frac{\\sqrt{2}}{4}, \\frac{\\sqrt{3}}{2})$.\n",
    "\n",
    "![Problem1 spherical wedge](Problem1 spherical wedge.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Irradiance [6 pts]\n",
    "\n",
    "Consider a camera looking at Lambertian wall with constant albedo, illuminated by a light source at infinity such that the radiance emitted by the wall is $L$ in all directions. The angle between the optical axis and the wallâ€™s surface normal is 60 degrees. The focal length of the camera is 50mm, and the pixels are 1mm by 1mm.\n",
    "\n",
    "![Problem2 irradiance](Problem2 irradiance.jpg)\n",
    "\n",
    "We will solve this for a pixel centered at the optical axis and also consider three distances along the line of sight between the pinhole and wall (1000mm, 2000mm, 4000mm). We will also walk through four steps and take advantage of small area approximations.\n",
    "\n",
    "1. Start with $d=1000\\text{ mm}$, \n",
    "    1. If the wall is parallel to the image plane at distance $d$, what is the area of the wall that projects to the central pixel?\n",
    "    1. Now, if the wall is tilted by 60 degrees, what is the area of the wall that projects to the pixel (you can treat this as a small area and can neglect an perspective distortion). Call this $dA_{1}$.\n",
    "    1. If we treat the pinhole as having a small area $dA_{2}$ that is parallel to the image plane, what is the power received at $dA_{2}$?\n",
    "    1. If all the power received at $dA_{2}$ passes through the pinhole and is received by the central pixel, what is the irradiance at the central pixel?\n",
    "1. What is the irradiance at the central pixel  for $d=2000\\text{ mm}$?\n",
    "1. What is the irradiance at the central pixel  for $d=4000\\text{ mm}$?\n",
    "1. What can you learn about image irradiance as a function fo distance from this example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Diffused Objects and Brightness [4 pts]\n",
    "\n",
    "We see a diffuse torus centered at the origin in an orthographic camera, looking down the z-axis. The parameters of the torus are shown in the Figure \"Problem3 torus\" and the albedo is $\\rho$.\n",
    "\n",
    "![Problem3 torus](Problem3 torus.png)\n",
    "\n",
    "This torus is illuminated by a distant point light source whose direction is $(0,0,1)$. There is no other illumination. \n",
    "\n",
    "What is the brightness at a point $(x, y)$ on the surface?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Occlusion, Umbra and Penumbra [2 pts]\n",
    "We have a square area source and a square occluder, both parallel to a plane. \n",
    "\n",
    "The edge lengths of the source and occluder are 2 and 4, respectively, and they are vertically above one another with their centers aligned. The distances from the occluder to the source and plane are both 3.\n",
    "\n",
    "1. What is the area of the umbra on the plane?\n",
    "\n",
    "2. What is the area of the penumbra on the plane?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Photometric Stereo, Specularity Removal [14 pts]\n",
    "\n",
    "The goal of this problem is to implement a couple of different algorithms that reconstruct a surface using the concept of photometric stereo.\n",
    "\n",
    "Additionally, you will implement the specular removal technique of Mallick et al., which enables photometric stereo reconstruction of certain non-Lambertian materials. \n",
    "\n",
    "You can assume a Lambertian reflectance function once specularties are removed, but the albedo is unknown and non-constant in the images.  \n",
    "\n",
    "Your program will take in multiple images as input along with the light source direction (and color when necessary) for each image.\n",
    "\n",
    "### Data\n",
    "Synthetic Images, Specular Sphere Images, Pear Images for Part 1, 2, 3: Available in `*.pickle` files (graciously provided by Satya Mallick) which contain\n",
    "\n",
    "* `im1`, `im2`, `im3`, `im4`... images.\n",
    "\n",
    "* `l1`, `l2`, `l3`, `l4`... light source directions.\n",
    "\n",
    "* `c` (when required) color of light source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Part 1: [6 pts]\n",
    "\n",
    "Implement the photometric stereo technique described in Forsyth and Ponce 2.2.4 (*Photometric Stereo: Shape from Multiple Shaded Images*) and the lecture notes. \n",
    "\n",
    "Your program should have two parts:\n",
    "\n",
    "1. Read in the images and corresponding light source directions, and estimate the surface normals and albedo map.\n",
    "\n",
    "1. Reconstruct the depth map from the surface normals. You can first try the naive scanline-based shape by integration method described in the book.  If this does not work well on real images, you can use the implementation of the Horn integration technique given below in `horn_integrate` function.\n",
    "\n",
    "Try using only `im1`, `im2` and `im4` first. Display your outputs as mentioned below.\n",
    "\n",
    "Then use all four images. (Most accurate).\n",
    "\n",
    "For each of the above cases you must output:\n",
    "\n",
    "1. The estimated albedo map.\n",
    "\n",
    "1. The estimated surface normals by showing both\n",
    "    1. Needle map, and\n",
    "    1. Three images showing components of surface normal.\n",
    "\n",
    "1. A wireframe of depth map.\n",
    "\n",
    "An example of outputs is shown in the Figure \"Problem5 example\".\n",
    "\n",
    "![Problem5 example](Problem5 example.png)\n",
    "\n",
    "Note: You will find all the data for this part in `synthetic_data.pickle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example: How to read and access data from a pickle\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pickle_in = open(\"synthetic_data.pickle\", \"rb\")\n",
    "data = pickle.load(pickle_in)\n",
    "# data = pickle.load(pickle_in, encoding=\"latin1\")\n",
    "\n",
    "# data is a dict which stores each element as a key-value pair. \n",
    "print(\"Keys: \" + str(data.keys()))\n",
    "\n",
    "# To access the value of an entity, refer it by its key.\n",
    "print(\"Image:\")\n",
    "plt.imshow(data[\"im1\"], cmap = \"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Light source direction: \" + str(data[\"l1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import convolve\n",
    "from numpy import linalg\n",
    "\n",
    "def horn_integrate(gx, gy, mask, niter):\n",
    "    '''\n",
    "    horn_integrate recovers the function g from its partial \n",
    "    derivatives gx and gy. \n",
    "    mask is a binary image which tells which pixels are \n",
    "    involved in integration. \n",
    "    niter is the number of iterations. \n",
    "    typically 100,000 or 200,000, \n",
    "    although the trend can be seen even after 1000 iterations.\n",
    "    '''    \n",
    "    g = np.ones(np.shape(gx))\n",
    "    \n",
    "    gx = np.multiply(gx, mask)\n",
    "    gy = np.multiply(gy, mask)\n",
    "    \n",
    "    A = np.array([[0,1,0],[0,0,0],[0,0,0]]) #y-1\n",
    "    B = np.array([[0,0,0],[1,0,0],[0,0,0]]) #x-1\n",
    "    C = np.array([[0,0,0],[0,0,1],[0,0,0]]) #x+1\n",
    "    D = np.array([[0,0,0],[0,0,0],[0,1,0]]) #y+1\n",
    "    \n",
    "    d_mask = A + B + C + D\n",
    "    \n",
    "    den = np.multiply(convolve(mask,d_mask,mode=\"same\"),mask)\n",
    "    den[den == 0] = 1\n",
    "    rden = 1.0 / den\n",
    "    mask2 = np.multiply(rden, mask)\n",
    "    \n",
    "    m_a = convolve(mask, A, mode=\"same\")\n",
    "    m_b = convolve(mask, B, mode=\"same\")\n",
    "    m_c = convolve(mask, C, mode=\"same\")\n",
    "    m_d = convolve(mask, D, mode=\"same\")\n",
    "    \n",
    "    term_right = np.multiply(m_c, gx) + np.multiply(m_d, gy)\n",
    "    t_a = -1.0 * convolve(gx, B, mode=\"same\")\n",
    "    t_b = -1.0 * convolve(gy, A, mode=\"same\")\n",
    "    term_right = term_right + t_a + t_b\n",
    "    term_right = np.multiply(mask2, term_right)\n",
    "    \n",
    "    for k in range(niter):\n",
    "        g = np.multiply(mask2, convolve(g, d_mask, mode=\"same\")) + term_right\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def photometric_stereo(images, lights, mask):\n",
    "    '''\n",
    "    your implementaion\n",
    "    '''\n",
    "    albedo = np.ones(images[0].shape)\n",
    "    normals = np.dstack((np.zeros(images[0].shape),\n",
    "                         np.zeros(images[0].shape),\n",
    "                         np.ones(images[0].shape)))\n",
    "    H = np.ones(images[0].shape)\n",
    "    H_horn = np.ones(images[0].shape)\n",
    "    return albedo, normals, H, H_horn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "pickle_in = open(\"synthetic_data.pickle\", \"rb\")\n",
    "data = pickle.load(pickle_in)\n",
    "# data = pickle.load(pickle_in, encoding=\"latin1\")\n",
    "\n",
    "lights = np.vstack((data[\"l1\"], data[\"l2\"], data[\"l4\"]))\n",
    "# lights = np.vstack((data[\"l1\"], data[\"l2\"], data[\"l3\"], data[\"l4\"]))\n",
    "\n",
    "images = []\n",
    "images.append(data[\"im1\"])\n",
    "images.append(data[\"im2\"])\n",
    "# images.append(data[\"im3\"])\n",
    "images.append(data[\"im4\"])\n",
    "images = np.array(images)\n",
    "\n",
    "mask = np.ones(data[\"im1\"].shape)\n",
    "\n",
    "albedo, normals, depth, horn = photometric_stereo(images, lights, mask)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Following code is just a working example so you don't get stuck with any\n",
    "# of the graphs required. You may want to write your own code to align the\n",
    "# results in a better layout.\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# Stride in the plot, you may want to adjust it to different images\n",
    "stride = 15\n",
    "\n",
    "# showing albedo map\n",
    "fig = plt.figure()\n",
    "albedo_max = albedo.max()\n",
    "albedo = albedo / albedo_max\n",
    "plt.imshow(albedo, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# showing normals as three separate channels\n",
    "figure = plt.figure()\n",
    "ax1 = figure.add_subplot(131)\n",
    "ax1.imshow(normals[..., 0])\n",
    "ax2 = figure.add_subplot(132)\n",
    "ax2.imshow(normals[..., 1])\n",
    "ax3 = figure.add_subplot(133)\n",
    "ax3.imshow(normals[..., 2])\n",
    "plt.show()\n",
    "\n",
    "# showing normals as quiver\n",
    "X, Y, _ = np.meshgrid(np.arange(0,np.shape(normals)[0], 15),\n",
    "                      np.arange(0,np.shape(normals)[1], 15),\n",
    "                      np.arange(1))\n",
    "X = X[..., 0]\n",
    "Y = Y[..., 0]\n",
    "Z = depth[::stride,::stride].T\n",
    "NX = normals[..., 0][::stride,::-stride].T\n",
    "NY = normals[..., 1][::-stride,::stride].T\n",
    "NZ = normals[..., 2][::stride,::stride].T\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.gca(projection='3d')\n",
    "plt.quiver(X,Y,Z,NX,NY,NZ, length=0.02)\n",
    "plt.show()\n",
    "\n",
    "# plotting wireframe depth map\n",
    "H = depth[::stride,::stride]\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(X,Y, H.T)\n",
    "plt.show()\n",
    "\n",
    "H = horn[::stride,::stride]\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(X,Y, H.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part 2: [4 pts]\n",
    "Implement the specularity removal technique described in *Beyond Lambert: Reconstructing Specular Surfaces Using Color* (by Mallick, Zickler, Kriegman, and Belhumeur; CVPR 2005).  \n",
    "\n",
    "Your program should input an RGB image and light source color and output the corresponding SUV image.  \n",
    "\n",
    "Try this out first with the specular sphere images and then with the pear images.  \n",
    "  \n",
    "For each specular sphere and pear images, include\n",
    "\n",
    "1. The original image (in RGB colorspace).\n",
    "\n",
    "1. The recovered $S$ channel of the image.\n",
    "\n",
    "1. The recovered diffuse part of the image - Use $G = \\sqrt{U^2+V^2}$ to represent the diffuse part.\n",
    "\n",
    "Note: You will find all the data for this part in `specular_sphere.pickle` and `specular_pear.pickle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rot_mat(rot_v, unit=None):\n",
    "    '''\n",
    "    Takes a vector and returns the rotation matrix required to align the\n",
    "    unit vector(2nd arg) to it.\n",
    "    '''\n",
    "    if unit is None:\n",
    "        unit = [1.0, 0.0, 0.0]\n",
    "    \n",
    "    rot_v = rot_v/np.linalg.norm(rot_v)\n",
    "    uvw = np.cross(rot_v, unit) #axis of rotation\n",
    "\n",
    "    rcos = np.dot(rot_v, unit) #cos by dot product\n",
    "    rsin = np.linalg.norm(uvw) #sin by magnitude of cross product\n",
    "\n",
    "    #normalize and unpack axis\n",
    "    if not np.isclose(rsin, 0):\n",
    "        uvw = uvw/rsin\n",
    "    u, v, w = uvw\n",
    "\n",
    "    # Compute rotation matrix \n",
    "    R = (\n",
    "        rcos * np.eye(3) +\n",
    "        rsin * np.array([\n",
    "            [ 0, -w,  v],\n",
    "            [ w,  0, -u],\n",
    "            [-v,  u,  0]\n",
    "        ]) +\n",
    "        (1.0 - rcos) * uvw[:,None] * uvw[None,:]\n",
    "    )\n",
    "    \n",
    "    return R\n",
    "\n",
    "def RGBToSUV(I_rgb, rot_vec):\n",
    "    '''\n",
    "    your implementation which takes an RGB image and a vector encoding\n",
    "    the orientation of S channel wrt to RGB\n",
    "    '''\n",
    "    S = np.ones(I_rgb.shape[:2])\n",
    "    G = np.ones(I_rgb.shape[:2])\n",
    "    return S, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"specular_sphere.pickle\", \"rb\")\n",
    "data = pickle.load(pickle_in)\n",
    "# data = pickle.load(pickle_in, encoding=\"latin1\")\n",
    "\n",
    "# sample input\n",
    "S, G = RGBToSUV(data[\"im1\"], np.hstack((data[\"c\"][0][0],\n",
    "                                        data[\"c\"][1][0],\n",
    "                                        data[\"c\"][2][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: [4 pts]\n",
    "Combine parts 1 and 2 by running your photometric stereo code on the diffuse components of the specular sphere and pear images. \n",
    "\n",
    "For comparison, run your photometric stereo code on the original images (converted to grayscale) as well. You should notice erroneous \"bumps\" in the resulting reconstructions, as a result of violating the Lambertian assumption.\n",
    "\n",
    "For each specular sphere and pear image sets, using all the four images, include:\n",
    "\n",
    "1. The estimated albedo map (original and diffuse)\n",
    "\n",
    "1. The estimated surface normals (original and diffuse) by showing both\n",
    "\n",
    "    1. Needle map, and\n",
    "    1. Three images showing components of surface normal\n",
    "    \n",
    "1. A wireframe of depth map (original and diffuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# You may reuse the code for photometric_stereo here.\n",
    "# Write your code below to process the data and send it to photometric_stereo\n",
    "# and display the albedo, normals and depth maps.\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6: Surface Rendering [10 pts]\n",
    "\n",
    "In this portion of the assignment we will be exploring different methods of approximating local illumination of\n",
    "objects in a scene. As discovered in the photometeric stereo portion of this homework, we know that different light\n",
    "models work better with different view, illumination sources and materials. This last section of the homework will be an exercise in rendering surfaces. Here, you need use the surface normals from Part 3 of Problem 5 to calculate the image intensity of the specular sphere and pear, with various light sources, different materials, and using a number of illumination models. For the sake of simplicity, multiple reflections of light rays, and occlusion of light rays due to object/scene can be ignored.\n",
    "\n",
    "### Data\n",
    "\n",
    "The surface normals of the specular sphere and the pear from Part 3 of Problem 5. For comparison, You should display the rendering results for both normals calculated from the original image and the diffuse components.\n",
    "\n",
    "Assume that the albedo map is uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambertian Illumination\n",
    "\n",
    "One of the simplest models available to render 3D objections with illumination is the Lambertian model. This\n",
    "model finds the apparent brightness to an observer using the direction of the light source $\\mathbf{L}$ and the normal\n",
    "vector on the surface of the object $\\mathbf{N}$. The brightness intensity at a given point on an objectâ€™s surface, $\\mathbf{I_d}$, with\n",
    "a single light source is found using the following relationship:\n",
    "\n",
    "$$\\mathbf{I_d} = \\mathbf{L} \\cdot \\mathbf{N} (I_l\\mathbf{C})$$\n",
    "\n",
    "where, $\\mathbf{C}$ and $I_l$ are the the color and intensity of the light source respectively.\n",
    "\n",
    "### Phong Illumination\n",
    "\n",
    "One major drawback of Lambertian illumination is that it only considers the diffuse light in its calculation of\n",
    "brightness intensity. One other major component to illumination rendering is the specular component. The\n",
    "specular reflectance is the component of light that is reflected in a single direction, as opposed to all directions,\n",
    "which is the case in diffuse reflectance. One of the most used models to compute surface brightness with specular\n",
    "components is the Phong illumination model. This model combines ambient lighting, diffused reflectance as well\n",
    "as specular reflectance to find the brightness on a surface. Phong shading also considers the material in the scene\n",
    "which is characterized by four values: the ambient reflection constant ($k_a$), the diffuse reflection constant ($k_d$),\n",
    "the specular reflection constant ($k_s$) and $\\alpha$ the Phong constant, which is the â€˜shininessâ€™ of an object. Furthermore,\n",
    "since the specular component produces â€˜raysâ€™, only some of which would be observed by a single observer, the\n",
    "observerâ€™s viewing direction ($\\mathbf{V}$) must also be known. For some scene with known material parameters with $M$\n",
    "light sources the light intensity $\\mathbf{I}_{phong}$ on a surface with normal vector $\\mathbf{N}$ seen from viewing direction $\\mathbf{V}$ can be\n",
    "computed by:\n",
    "\n",
    "$$\\mathbf{I}_{phong} = k_{a}\\mathbf{I}_{a} + \\sum_{m\\in M}\\left\\{k_d(\\mathbf{L}_{m}\\cdot\\mathbf{N})\\mathbf{I}_{m,d} + k_{s}(\\mathbf{R}_{m}\\cdot\\mathbf{V})^{\\alpha}\\mathbf{I}_{m,s}\\right\\}\\text{,}$$\n",
    "\n",
    "$$\\mathbf{R}_{m} = 2\\mathbf{N}(\\mathbf{L}_{m}\\cdot\\mathbf{N}) - \\mathbf{L}_{m}\\text{,}$$\n",
    "\n",
    "where $\\mathbf{I}_{a}$, is the color and intensity of the ambient lighting, $\\mathbf{I}_{m,d}$ and $\\mathbf{I}_{m,s}$ are the color values for the diffuse and\n",
    "specular light of the $m$th light source.\n",
    "\n",
    "### Rendering\n",
    "\n",
    "Please complete the following:\n",
    "\n",
    "1. Write the function `lambertian()` that calculates the Lambertian light intensity given the light direction $\\mathbf{L}$ with color and intensity $\\mathbf{C}$ and $I_l = 1$, and normal vector $\\mathbf{N}$. Then use this function in a program that calculates and displays the specular sphere and the pear using each of the two lighting sources found in Table 1. *Note: You do not need to worry about material coefficients in this model.*\n",
    "\n",
    "1. Write the function `phong()` that calculates the Phong light intensity given the material constants $(k_a, k_d, k_s, \\alpha)$, $\\mathbf{V} = (0, 0, 1)^\\top$, $\\mathbf{N}$ and some number of $M$ light sources. Then use this function in a program that calculates and displays the specular sphere and the pear using each of the sets of coefficients found in Table 2 with each light source individually, and both light sources combined.\n",
    "\n",
    "*Hint: To avoid artifacts due to shadows, ensure that any negative intensities found are set to zero.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Table 1: Light Sources\n",
    "\n",
    "| $m$ | Location | Color (RGB)  |\n",
    "| - | ----------- | ----- |\n",
    "| 1 | $(-\\tfrac{1}{3},\\tfrac{1}{3},\\tfrac{1}{3})^{\\top}$ | $(1,1,1)$ |\n",
    "| 2 | $(1,0,0)^{\\top}$     | $(1,.5,.5)$ |\n",
    "\n",
    "Table 2: Material Coefficients\n",
    "\n",
    "| Mat. | $k_a$ | $k_d$ | $k_s$ | $\\alpha$ |\n",
    "| - | -------- | ----- | ----- | -------- |\n",
    "| 1 | $0$ | $0.1$ | $0.75$ | $5$ |\n",
    "| 2 | $0$ | $0.5$ | $0.1$ | $5$ |\n",
    "| 3 | $0$ | $0.5$ | $0.5$ | $10$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Lambertian model [4 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambertian(normals, lights, color, intensity, mask):\n",
    "    '''Your implementation'''\n",
    "    image = np.ones((normals.shape[0], normals.shape[1], 3))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the rendering results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Phong model [6 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phong(normals, lights, color, material, view, mask):\n",
    "    '''Your implementation'''\n",
    "    image = np.ones((normals.shape[0], normals.shape[1], 3))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the rendering results"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "216px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
