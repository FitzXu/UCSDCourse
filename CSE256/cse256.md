# CSE 256 NLP

* Instructor: Ndapa Nakashole (ndap@eng.ucsd.edu, CSE 4108)
	* Professor has been at UCSD since January 2017
* TAs
	* Sharathabhinav Dharmaji sdharmaj@ucsd.edu
	* Tayal, Piyush ptayal@ucsd.edu 
	* (3rd TA coming soon...)

## Useful Links

* [Website](http://cseweb.ucsd.edu/~nnakashole/teaching/256_sp19.html)
	* [Undergrad old website](http://cseweb.ucsd.edu/~nnakashole/teaching/190_wi18.html)
* [Numpy tutorial](http://cs231n.github.io/python-numpy-tutorial/)
* [TritonEd](https://tritoned.ucsd.edu)
	* Has links to slides

## 04/01/19: Week 1 Monday: Introduction

__Outline__

* What is Natural Language Processing?
* Sample of NLP applications
* Why is language understanding difficult?
* Examples of state-of-the-art NLP methods (mostly deep learning)
* Course logistics

### 1. What is Natural Language Processing (NLP)?

* (assignment 1 will involve translations)
* NL = Natural Languages
* NLP develops algorithms for
	* Analysis of language (NL to some useful output): e.g., text classification, question answering, ...
	* Generation of language (NL to NL; image to NL, etc): e.g., summarization, image captioning, MT (machine translation)
	* Acquisition of a representation (data to some representation): e.g., learning word embeddings, sentence embeddings
* (thiss class will host kaggle competition. rank won't affect grade. to rank highly, consider playing around with word embeddings)

__Levels of Language Analysis__

(Speech: Phonetic/Phonological Analysis, Text: OCR/Tokenization) ⟶ Morphological Analysis ⟶ Syntactic Analysis ⟶ Semantic Interpretation ⟶ Discourse Processing

* (In English, morphology is easy. In german, grammar is nontrivial)
* (This class will cover syntactic analysis and semantic interpretation)
* (2 assignments dedicated to syntactic analysis, seq2seq and parse trees using DP algorihms)
* (Won't be doing discourse processing in this class)
	* pragmatics = taking into account context to figure out meaning)
	* Take a linguistics class to cover everything eelse

__What is special about human language?__

* A human language is a system specifically constructed to convey the speaker/writer's meaning
	* Not just an environment signal, it's a deliberate communication
* A human language is a diverse/symbolic/categorical signaling system
	* Thus we communicate with people via symbols
	* With very minor exceptions for expressive signaling ("I loooove it.", "Whoomppaaa")
* The large vocabulary, symbolic encoding of words creates a problem for machine learning — sparsity!

### 2. A sample of NLP Applications

* Basic language processing tasks
	* spellcheckers
	* keyword search: finding synonyms, etc

__Question answers__

* More than search
	* Requires precise answers than just documents
* QA complexity
	* Can be easy: Who is the Prime Minister of the UK?
	* Can be harder: How many capital cities are also the largest cities of their countries?
	* Can be open ended: How did the 2008 financial crisis happen?
* State of the art
	* Success on factoid questions, even when text is not a perfect match
* Question Answering: Watson
	* D. Ferrucci et al: Building Watson ...
		* Information Retrieval with NLP

__Knowledge Discovery and Information Extraction__

* Generation of Knowledge Graphs (KGs)
	* Collect and store world knowledge in a format that is suitable for machine inference and reasoning
* Example companies: yago, DBpedia, Google Knowledge Graph, Cyc, Freebase, Microsoft Satori
	* NELL = Never-Ending Language Learning

__NELL  Knowledge Graph Fragment__

(see figure)

* \[Carlson et al., 2010\]
* \[Mitchell et al., 2015\]

__Names vs Entities__

* Reference resolution: find mentions that refer to the same entity
	* Input: ...
	* Correct: ...
	* Guess: ...
* Anaphora
	* Ex: The dog chased the cat, which ran up a tree. It waited at the top.
	* nontrivial problem

__Machine Translation___

Ex: Der Spiegel (German newspaper)

__End-to-End Example__

* Personal assistants contain
	* Speech recognition
	* Language analysis
	* Dialog processing
	* Text to speech

__NLP in industry is taking off__

* Search (written and spoken)
* Online advertisement matching
* Automated/assisted translation
* Sentiment anylsis for marking or finance.trading
* Speech recognition
* Chatbots/Dialog agents
	* Automating customer support
	* Controlling devices
	* Ordering goods
	* ...
	* (might not be covered in class)

### 3. Why is language understanding difficult?

* Human languages are ambiguous (unlike programming and other formal languages)
* Richness: any meaning may be expressed many ways
* Human language interpretation depends on real world, common sense, and contextual knowledge
	* (common sense in AI is still a work in progress)
* Linguistic diversity across languages, dialects, genres, styles, ...

__xkcd comic__

(formalizing language is impossible/complex)

__Ambiguity is at all Levels of NLP__

__Ambiguity__

* Every level of linguistic analysis comes with its own ambiguities
* Syntax
	* For POS tagging: multiple POS tags (bark - noun, bark - verb)
	* For parsing: multiple parse trees are correct
* Semantics
	* Synonymy: different words same meaning
	* Polysemy: same word different meanings
	* Multiword expressions: make a decision, take out, make up, ...

__Our focus: Syntax & Semantics__

* Syntax: What is grammatical
* Semantics: What does it mean
* Programming languages analogy
	* Syntax: no compiler errors
	* Semantics: no implementation bugs

__Syntax anaylsis and annotation__

(Stanford Parser example: "The boy wants to go to San Diego")

Sometimes syntax annotations are helpful. Provided a lot of improvements to accuracy before the rise of deep learning.

__Syntax annotations (II): phrase structure__

(see slides)

__Syntax annotations (III): dependency structure __

(CMU parser)

* Basic dependency representation forms a tree
	* Exactly one word is the head of the sentence (wants)
	* All other words depend on another word in the sentence
* Dependency relations
	* nsubj: subject (nominal)
	* pobj: object (of a preposition)

__Syntax uses__

* Part of speech:
	* Named entity recognition, relation etraction, MT, ...
	* (can help improve deep learning model)
* Dependency relations
	* Relation extraction, sentiment analysis
	* ...

__Prepositional Attachment Ambigity__

* sentence 1: Alice caught the butterfly with the spots
* sentence 2: Alice caught the butterfly with the net
* (need background knowledge to determine the correct parse tree)
* Is syntax enough to disambiguate?

__Semantic Ambiguity__

* Even correct tree syntactic anylsis don't fully nail down meaning
	* John's boss said he was doing better
* Real newspaper headlines/tweets
	* The pope's baby steps on gays
	* Stolen painting found by tree
	* Kids make nutritious snacks
	* Local high school dropouts cut in half

__Factors Changing the NLP Landscape (Hirschberg and Manning, Science 2015)__

* Increases in computing power (GPUs)
* The rise of the web, then the social web
* Advances in machine learning (lesser extent)
* Advances in understanding of languge in social contexts

### 4. Examples of State-of-the-art NLP methods

* Deep NLP = Deep Learning + NLP
	* Combine ideas and goals of NLP with using representation learning and deep learning methods to solve them
* Several big improvements in recent years in NLP with different
	* Levels: speech, words, syntax, semantics
	* Tools: parts-of-speech, entities, parsing
	* Aplications: machine translation, sentiment analysis, dialogue agents, question answering

__Word meaning as word vector__

(idea has been around for a while, but now we have much more data, so more accurate)

__Dialogue agents / Reponse Generation__

* A simple, successful example is the auto-replies available in the Google Inbox app
* An application of the powerful, general technique of Neural Language Models, which are an instance of Recurrent Neural Networks

__Persona-based Neural Dialog Model (Li et al 2017)__

(Stanford NLP group)

* Model each speaker in embedding space
	* (see figure)
* Also model who the speaker is speaking to in speaker-addressee model

__Instructable agents__

* (chatbots)
* Instructable agents (Azaria et al AAAI 2016)
	* ...
* "I'm stuck in traffic and will be late" ...
* Teaching the system that "drop a note to Bill" has the same meaning as "send an email to Bill"

__Neural Machine Translation__

* Source sentence is mapped to vector, then output sentence generated
	* (see figure)
	* Now live for some languages in Google Translate (etc.), with big error reductions

__Neural Question Answering__

* (another example where deep learning made a huge impact)
* (see figure)
	* Input story:
		* 1: Sam moved to garden, 2: Sam went to kitchen, 3: Sam drops apple there
	* Question:
		* Where is Sam?

### 5. Course Logistics

* Fundamental ideas used in key NLP components
* Some big picture understanding of human languages and the difficulties in understnading and producing them
* An understanding of and ability to build systems for some of the major problems in NLP:
	* text classification, parsing, machine translation, ...
* (this year: replaced midterm with a final project)

__Course outline__

* 1) Probabilistic language models
	* Define probability distributions over text sequences
	* (Widely applicable. will cover the math we need upfront)
* 2) Text classifiers
	* Infer attributes of a piece of text by "reading" it
* 3) Semantics
	* Represet meaning
* 4) Sequence models
	* Convert sequences into other sequences
* 5) Parsing
	* Parse sentences into syntactic representations
* 6) Machine translation
	* Map text in one language to text in another

__Grading policy__

* Grade
	* 80% (5) programing assignments
	* 20% final project
* Participation
	* Attend classes
	* Ask questions; answer questions in class and on Piazza

__Readings__

* Reference texts
	* Jacob Eisenstein Book
		* (really clear)
	* Michael Collin notes
* Research articles
* Other texts: Jarfsky and Martin
	* (really good, but missing modern stuff)
* Instructor: Ndapa Nakashole (ndap@eng.ucsd.edu, CSE 4108)
* TAs
	* Sharathabhinav Dharmaji sdharmaj@ucsd.edu
	* Tayal, Piyush ptayal@ucsd.edu
	* ...
* Course website
	* http://cseweb.ucsd.edu/~nnakashole/teaching/256_sp.html
