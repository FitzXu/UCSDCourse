# CSE255

## Before class

+ watch video and answer the poll (not important)
+ Quizs: no announcement usually at the end of class

## 04/02/19 Week 1 Tuesday

### 1. Data Science is what

+ Interpretability

  In some cases , interpretation is important.

  + because give more confidence
  + can justify the prediction with evidence

+ Inter-disciplinary

  Data Science have two types of researchers:

  + Domain experts: collect data, annotate, formulate the question
  + Methods experts: math/computer methods to answer question

###2. Examples of data science

+ Caltrans: PeMS - measure performance of highway with sensors
+ eviction rate
+ goole flu: how many people search for 'flu' with google
+ echo location: data is large (12TB, processing takes 2 weeks)
  + AWS cloud is helpful and cheap to compute
  + But transfer the data from local to cloud taks time and expensive

###3. What to learn

+ basic statistics
+ principal components

## 04/04/2019 Week1 Thursday

+ Learn what
  - parallel computing
  - distributed

###1. why it is slow to deal with large dataset

+ process of multiplicationt
  + read operands into registers
  + compute the result and store into another register
  + write back to the memory
+ latency: read/write$\geq$ compute
  + Improving lantency is one way to speed up
  + another way is to improve thoughout e.g. use many computers parallel
+ storage types
  - main memory
  - spinning disk
  - remote computer (depends on the network)

### 2. Access locality

â€‹	Accessing address in neighboring pages means **good locality**

+ Memory access locality
  + tempoeral locality: repeat the access again and again

    + example: $f_{\theta}(x)$ given with $x_1,x_2,...$
    + keep $\theta$ in cache

  + spcatial locality: access elements close to each other

    two examples to store sequence 

    + linked list: the location is not continuous, always move far way to locate the data
    + indexed array: faster

+ Cache
  + splitted into blocks/lines
  + Cache and memory
    + when expect to access an address
      + check if the address is in cache
  + Type of locality
    + temporal locality
    + spatial locality

###3. The memory hierarchy

+ Architecture
  + top hierarchy: small but fast storage close to CPU
  + bottom hierarchy: large but slow further from CPU
  + cache is transfer between levels

+ Row-wise vs column-wise scanning

  +  numpy array row major order

+ Measure memory letency

  + Histogram easy to be poluted

+ Clusters: multiple computers connected by ethernet

  + spark could help to utilize those computers and abstract them into one array

  ![image-20190405111123841](/Users/chenguanghao/Library/Application Support/typora-user-images/image-20190405111123841.png)

### 4. Large scale computing

+ History

  + Super computer: specialized hardwares, expensive, designed to solve specialized task
  + data center
    + physical aspect of "cloud computing"
    + large amount of commodity computers (computers we use everyday)
  + Google 2003 by Larry Page and Sergey Brin
    + split file into chunks
    + Copy chunks across many chunk servers (distributed systems)

+ HDFS

  + Architecture
    + Files are splitted into chunks and store into different servers
    + **Master** knows which chunk is in which server

  ![image-20190405111842132](/Users/chenguanghao/Library/Application Support/typora-user-images/image-20190405111842132.png)

  + Properties
    + low cost for storage
    + locality: storage is close to cpu
      + E.G. If we want to sum all the elements in file 1
        + A better way is to use chunk1 from server1 and chunk2 from server2
        + to avoid processing chunk2 after chunk1
    + redundancy: recover from the copy
    + modern architecture has a secondary master 
      + in case the master is failed

+ Map-Reduce

  + Definition

    + a description to compute aceoss many computers
    + a kind of abstraction for HDFS

  + Spark and hadoop

    + hadoop uses shared file system
    + spark uses shared memory

  + Map

    + Example compute the power of list [0,1,2,3]

      + Traditional way 

        + For loop
        + List comprehension

      + Map

        + most important thing is: not specified about the **computation order**

        ```python
        map(lambda x:x*x, L)
        ```

        

  + Reduce

    + Example: compute the sum of list [1,2,3,4]

      + Traditional ways

        + sum(L)
        + for loop

      + reduce

        + Again: the order is not specified

        ```python
        reduce(lambda (x,y):x+y,L)
        ```

  + Map and Reduce

    + Example: compute the sum of squares of list [1,2,3,4]

      + Map-reduce

        ```python
        reduce(lambda x,y:x+y, map(lambda i:i**2,L))
        ```

+ Spark and Map-Reduce

  + History
    + Google File System 2003
    + Google MapReduce 2004
    + Apache Hadoop , 2006
      + File System: GFS -> HDFS
      + Compute System: Google MapReduce -> Hadoop MapReduce
  + Spark
    + Built in Java
    + can programmed in Java but the code tends to be long
    + New language Scala, but the language is not popular
  + 



## Week2

### 1. Spark context

+ using regular function

+ ```python
  def fun():
  	pass
  sc.reduce(fun)
  ```

### 2. Changing the number of workers

```python
sc = SparkContext(master="local[3]")
```

+ running more workers on only one core doesn't help
  + always exchangingthe context among diff workers
  + 

